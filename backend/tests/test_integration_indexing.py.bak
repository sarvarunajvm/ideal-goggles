"""
Integration tests for the complete indexing pipeline.
"""

import pytest
import asyncio
import tempfile
import shutil
from pathlib import Path
from unittest.mock import patch, MagicMock, AsyncMock
from PIL import Image
import numpy as np
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from workers.crawler import PhotoCrawler
from workers.thumbnail_worker import ThumbnailWorker
from workers.exif_extractor import ExifExtractor
from workers.embedding_worker import EmbeddingWorker
from workers.ocr_worker import OCRWorker
from workers.face_worker import FaceWorker
from db.connection import DatabaseConnection
from services.faiss_manager import FaissManager


class TestIndexingPipeline:
    """Integration tests for the complete indexing pipeline."""

    @pytest.fixture
    async def temp_photo_dir(self):
        """Create temporary directory with test photos."""
        temp_dir = tempfile.mkdtemp()

        # Create test images
        for i in range(5):
            img = Image.new('RGB', (800, 600), color=(i*50, i*50, i*50))
            img.save(os.path.join(temp_dir, f'test_photo_{i}.jpg'))

        yield temp_dir

        # Cleanup
        shutil.rmtree(temp_dir)

    @pytest.fixture
    async def test_database(self):
        """Create test database."""
        db_path = tempfile.mktemp(suffix='.db')
        db = DatabaseConnection(db_path)
        await db.initialize()

        yield db

        # Cleanup
        await db.close()
        os.remove(db_path)

    @pytest.mark.asyncio
    async def test_complete_indexing_flow(self, temp_photo_dir, test_database):
        """Test the complete indexing flow from crawling to search."""
        # Initialize components
        crawler = PhotoCrawler(temp_photo_dir)
        thumbnail_worker = ThumbnailWorker()
        exif_extractor = ExifExtractor()

        # Mock ML components for faster testing
        with patch('workers.embedding_worker.load_model') as mock_model:
            mock_model.return_value = MagicMock()
            embedding_worker = EmbeddingWorker()
            embedding_worker.generate_embedding = AsyncMock(
                return_value=np.random.rand(512).astype(np.float32)
            )

        # Step 1: Crawl photos
        photos = await crawler.crawl()
        assert len(photos) == 5

        # Step 2: Process each photo
        processed_photos = []
        for photo_path in photos:
            # Generate thumbnail
            thumbnail = await thumbnail_worker.generate(photo_path)
            assert thumbnail is not None

            # Extract EXIF data
            exif_data = await exif_extractor.extract(photo_path)
            assert isinstance(exif_data, dict)

            # Generate embedding
            embedding = await embedding_worker.generate_embedding(photo_path)
            assert embedding.shape == (512,)

            processed_photos.append({
                'path': photo_path,
                'thumbnail': thumbnail,
                'exif': exif_data,
                'embedding': embedding
            })

        # Step 3: Store in database
        for photo in processed_photos:
            photo_id = await test_database.insert_photo(
                file_path=photo['path'],
                thumbnail=photo['thumbnail'],
                metadata=photo['exif']
            )
            assert photo_id is not None

            await test_database.insert_embedding(
                photo_id=photo_id,
                embedding=photo['embedding']
            )

        # Step 4: Verify data was stored
        stored_photos = await test_database.get_all_photos()
        assert len(stored_photos) == 5

    @pytest.mark.asyncio
    async def test_incremental_indexing(self, temp_photo_dir, test_database):
        """Test incremental indexing with new files."""
        crawler = PhotoCrawler(temp_photo_dir)

        # Initial crawl
        photos = await crawler.crawl()
        initial_count = len(photos)

        # Add new photos
        for i in range(3):
            img = Image.new('RGB', (400, 300), color='red')
            img.save(os.path.join(temp_photo_dir, f'new_photo_{i}.jpg'))

        # Incremental crawl
        new_photos = await crawler.crawl_incremental(
            last_indexed_time='2024-01-01'
        )

        assert len(new_photos) == 3
        assert len(await crawler.crawl()) == initial_count + 3

    @pytest.mark.asyncio
    async def test_face_detection_pipeline(self, temp_photo_dir):
        """Test face detection in the indexing pipeline."""
        # Create photo with face (mock)
        face_photo = os.path.join(temp_photo_dir, 'face_photo.jpg')
        img = Image.new('RGB', (800, 600), color='white')
        img.save(face_photo)

        with patch('workers.face_worker.detect_faces') as mock_detect:
            mock_detect.return_value = [
                {
                    'bbox': [100, 100, 200, 200],
                    'confidence': 0.99,
                    'embedding': np.random.rand(128)
                }
            ]

            face_worker = FaceWorker()
            faces = await face_worker.detect_faces(face_photo)

            assert len(faces) == 1
            assert faces[0]['confidence'] > 0.9

    @pytest.mark.asyncio
    async def test_ocr_pipeline(self, temp_photo_dir):
        """Test OCR text extraction in the pipeline."""
        # Create photo with text (mock)
        text_photo = os.path.join(temp_photo_dir, 'text_photo.jpg')
        img = Image.new('RGB', (800, 600), color='white')
        img.save(text_photo)

        with patch('workers.ocr_worker.extract_text') as mock_ocr:
            mock_ocr.return_value = "Sample text from image"

            ocr_worker = OCRWorker()
            text = await ocr_worker.extract_text(text_photo)

            assert text == "Sample text from image"

    @pytest.mark.asyncio
    async def test_faiss_index_building(self, test_database):
        """Test building FAISS index from embeddings."""
        # Create mock embeddings
        embeddings = []
        photo_ids = []

        for i in range(100):
            embedding = np.random.rand(512).astype(np.float32)
            embeddings.append(embedding)
            photo_ids.append(i)

            await test_database.insert_embedding(
                photo_id=i,
                embedding=embedding
            )

        # Build FAISS index
        faiss_manager = FaissManager(dimension=512)
        faiss_manager.add_embeddings(embeddings, photo_ids)

        # Test search
        query_embedding = np.random.rand(512).astype(np.float32)
        results = faiss_manager.search(query_embedding, k=10)

        assert len(results) == 10
        assert all(isinstance(r['photo_id'], int) for r in results)
        assert all(0 <= r['distance'] <= 10 for r in results)

    @pytest.mark.asyncio
    async def test_error_handling_in_pipeline(self, temp_photo_dir):
        """Test error handling in the indexing pipeline."""
        # Create corrupted image
        corrupted_path = os.path.join(temp_photo_dir, 'corrupted.jpg')
        with open(corrupted_path, 'wb') as f:
            f.write(b'not a valid image')

        thumbnail_worker = ThumbnailWorker()

        # Should handle error gracefully
        thumbnail = await thumbnail_worker.generate(corrupted_path)
        assert thumbnail is None

    @pytest.mark.asyncio
    async def test_concurrent_indexing(self, temp_photo_dir):
        """Test concurrent processing of multiple photos."""
        # Create more test images
        for i in range(20):
            img = Image.new('RGB', (200, 200), color='blue')
            img.save(os.path.join(temp_photo_dir, f'concurrent_{i}.jpg'))

        crawler = PhotoCrawler(temp_photo_dir)
        photos = await crawler.crawl()

        # Process concurrently
        async def process_photo(photo_path):
            thumbnail_worker = ThumbnailWorker()
            return await thumbnail_worker.generate(photo_path)

        tasks = [process_photo(photo) for photo in photos[:10]]
        results = await asyncio.gather(*tasks)

        assert len(results) == 10
        assert all(r is not None for r in results)